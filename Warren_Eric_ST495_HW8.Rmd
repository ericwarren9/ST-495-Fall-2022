---
title: "ST 495 HW8"
author: "Eric Warren"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      fig.height = 7,
                      fig.width = 10)
```

## Honor Code {.tabset}

I have neither given nor received unauthorized assistance on this test or assignment. -**Eric Warren**

### Problem 1 {.tabset}

#### Part A

Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states using `USArrests` data.
```{r}
library(ISLR)
library(tidyverse)
head(USArrests) # Look at initial things for the data

# Do the clustering
arrests_hc_complete <- hclust(dist(USArrests, method = "euclidean"),
                              method = "complete")

# Plot the clustering
plot(arrests_hc_complete, 
     main = "Complete Linkage USArrests Clustering", 
     xlab = "Variables to Cluster", 
     sub = "Eric Warren",
     cex = .69)
```

Here we can see some states that are clustered together. For example, Florida and North Carolina are very similar.

#### Part B

Cut the dendrogram at a height that results in three distinct clusters.
```{r}
arrest_clusters <- cutree(arrests_hc_complete, 3)

# This pulls out the names of the states
cluster_1 <- arrest_clusters[arrest_clusters == 1] 
cluster_2 <- arrest_clusters[arrest_clusters == 2]
cluster_3 <- arrest_clusters[arrest_clusters == 3]

# Plot the clustering
library(ape)
colorsUsed = c("black", "red", "blue")
plot(as.phylo(arrests_hc_complete), 
     tip.color = colorsUsed[cutree(arrests_hc_complete, 3)],
     main = "Complete Linkage")
```

Here we can see the three clusters that are present based on their arrest statistics. The first cluster includes the states of `r names(cluster_1)`. The second cluster includes the states of `r names(cluster_2)`. Lastly, the third cluster includes the states of `r names(cluster_3)`.

#### Part C

Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.
```{r}
arrests_scaled <- scale(USArrests)
clusters_scaled <- hclust(dist(arrests_scaled, method = "euclidean"),
                          method = "complete")
plot(clusters_scaled, 
     main = "Hierarchical Clustering with Scaled USArrests Data", 
     xlab = "Variables Clustered", 
     sub = "Eric Warren",
     cex = .69)
```

Now we can see how some states are very similar when modeled like this. For example, Iowa and New Hampshire are very similar. Meanwhile, Alaska is not similar to many states.

#### Part D

We are going to compare the non-scaled vs. scaled clustering.
```{r}
par(mfrow = c(1, 2))
plot(arrests_hc_complete, 
     main = "Complete Linkage USArrests Clustering", 
     xlab = "Variables to Cluster", 
     sub = "Eric Warren",
     cex = .69)
plot(clusters_scaled, 
     main = "Hierarchical Clustering with Scaled USArrests Data", 
     xlab = "Variables Clustered", 
     sub = "Eric Warren",
     cex = .69)
```

What effect does scaling the variables have on the hierarchical clustering obtained? 
- Scaling the variables makes outliers and differences show more. When having everything on a scale, these differences as the same weight across all the predictor variables. Also, without scaling variables like `Assault` will have a larger say on the groups since its value is much higher. Scaling eliminates this problem.

In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed?
- I believe the variables should be scaled before finding groups because everything will be equally weighted; moreover, the `Assault` variable will not have a bigger influence on the groups. If the variables had the same scales then scaling would not be necessary but in this case where they are very different scaling is needed. 

### Problem 2

#### Part A

Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables.
```{r}
set.seed(9)
random_data <- rbind(matrix(rnorm(20*50, mean = 0), nrow = 20),
                     matrix(rnorm(20*50, mean = 0.9), nrow = 20),
                     matrix(rnorm(20*50, mean = -0.9), nrow = 20))
```

#### Part B

Perform PCA on the 60 observations and plot the first two principal component score vectors. Use a different color to indicate the observations in each of the three classes. If the three classes appear separated in this plot, then continue on to part (c). If not, then return to part (a) and modify the simulation so that there is greater separation between the three classes. Do not continue to part (c) until the three classes show at least some separation in the first two principal component score vectors.
```{r}
pca_data <- prcomp(random_data)$x

group1 <- rep(1, 20)
group2 <- rep(2, 20)
group3 <- rep(3, 20)
plot(pca_data[ , 1:2], 
     col = c(group1, group2, group3),
     main = "PCA Vectors of the Different Groups",
     sub = "Eric Warren")
legend(2, -2.5, legend = c("Group 1", "Group 2", "Group 3"), 
       fill = c("black","red", "green")
)
```

Here we can see the three distinct different groups of data that we have.

#### Part C

Perform K-means clustering of the observations with K = 3. How well do the clusters that you obtained in K-means clustering compare to the true class labels?
```{r}
k_means_data <- kmeans(random_data, centers = 3)
true_group <- c(group1, group2, group3)
table(k_means_data$cluster, true_group)
```

As we can see all groups are perfectly clustered.

#### Part D

Perform K-means clustering with K = 2.
```{r}
k_means_data <- kmeans(random_data, centers = 2)
table(k_means_data$cluster, true_group)
```

In the case of this data, We can see that the two extremes are separated from each other and then the middle group is mostly going with the positive value side rather than the other.

#### Part E

Perform K-means clustering with K = 4.
```{r}
k_means_data <- kmeans(random_data, centers = 4)
table(k_means_data$cluster, true_group)
```

We can see that most of the observations are in two clusters while the last two seem like they are not really needed.

#### Part F

Now perform K-means clustering with K = 3 on the first two principal component score vectors, rather than on the raw data. That is, perform K-means clustering on the 60 Ã— 2 matrix of which the first column is the first principal component score vector, and the second column is the second principal component score vector.
```{r}
k_means_data <- kmeans(pca_data[, 1:2], centers = 3)
table(k_means_data$cluster, true_group)
```

As we can see, the groups are perfectly separated again.

#### Part G

Using the `scale()` function, perform K-means clustering with K = 3 on the data after scaling each variable to have standard deviation one. 
```{r}
set.seed(9)
k_means_data <- kmeans(scale(random_data), centers = 3)
table(k_means_data$cluster, true_group)

plot(scale(random_data), 
     col = c(group1, group2, group3),
     main = "Scaling of the Different Groups and Classifying Them",
     sub = "Eric Warren")
legend("topleft", legend = c("Group 1", "Group 2", "Group 3"), 
       fill = c("black", "red", "green"))
```

The groups are not perfectly separated. So the results are the same as they were in Part B. We can see scaling the variables makes them a lot closer to each other and can causes issues with the groupings.

### Problem 3 {.tabset}

#### Make Function

Here we are going to make a function we can use based on algorithm 12.1.
```{r}
fit.svd <- function(x, m = 1) {
   svdob <- svd(x)
   with(svdob,
       u[ , 1:m, drop = FALSE] %*%
       (d[1:m] * t(v[, 1:m, drop = FALSE]))
     )
}
```

#### Test With Boston Data {.tabset}

Here we are going to use the `Boston` data to start our analysis.
```{r}
library(MASS)
scaled_boston <- data.matrix(scale(Boston))
```

##### 5% Omit Rate

Here we will keep track of our error rates and number of iterations. This is for 5%.
```{r}
# Omit number of variables
nomit <- round(.05 * nrow(scaled_boston), 0)
set.seed(9)
ina <- sample(nrow(scaled_boston), nomit)
inb <- sample(1:ncol(scaled_boston), nomit, replace = TRUE)
Xna <- scaled_boston
index.na <- cbind(ina, inb)
Xna[index.na] <- NA

# Initialize xhat
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]

# Initialize thresholds
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)

while(rel_err > thresh) {
    iter <- iter + 1
    # Step 2(a)
    Xapp <- fit.svd(Xhat, m = 8)
    # Step 2(b)
    Xhat[ismiss] <- Xapp[ismiss]
    # Step 2(c)
    mss <- mean(((Xna - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss) / mss0
    mssold <- mss
    cat("Iter:", iter, "MSS:", mss,
      "Rel. Err:", rel_err, "\n")
    rel_err_5 <- data.frame(iter, mss, rel_err)
    rel_err_5$missing_percent <- .05
    }
```

##### 10% Omit Rate

Here we will keep track of our error rates and number of iterations. This is for 10%.
```{r}
# Omit number of variables
nomit <- round(.10 * nrow(scaled_boston), 0)
set.seed(9)
ina <- sample(nrow(scaled_boston), nomit)
inb <- sample(1:ncol(scaled_boston), nomit, replace = TRUE)
Xna <- scaled_boston
index.na <- cbind(ina, inb)
Xna[index.na] <- NA

# Initialize xhat
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]

# Initialize thresholds
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)

while(rel_err > thresh) {
    iter <- iter + 1
    # Step 2(a)
    Xapp <- fit.svd(Xhat, m = 8)
    # Step 2(b)
    Xhat[ismiss] <- Xapp[ismiss]
    # Step 2(c)
    mss <- mean(((Xna - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss) / mss0
    mssold <- mss
    cat("Iter:", iter, "MSS:", mss,
      "Rel. Err:", rel_err, "\n")
    rel_err_10 <- data.frame(iter, mss, rel_err)
    rel_err_10$missing_percent <- .10
    }
```

##### 15% Omit Rate

Here we will keep track of our error rates and number of iterations. This is for 15%.
```{r}
# Omit number of variables
nomit <- round(.15 * nrow(scaled_boston), 0)
set.seed(9)
ina <- sample(nrow(scaled_boston), nomit)
inb <- sample(1:ncol(scaled_boston), nomit, replace = TRUE)
Xna <- scaled_boston
index.na <- cbind(ina, inb)
Xna[index.na] <- NA

# Initialize xhat
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]

# Initialize thresholds
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)

while(rel_err > thresh) {
    iter <- iter + 1
    # Step 2(a)
    Xapp <- fit.svd(Xhat, m = 8)
    # Step 2(b)
    Xhat[ismiss] <- Xapp[ismiss]
    # Step 2(c)
    mss <- mean(((Xna - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss) / mss0
    mssold <- mss
    cat("Iter:", iter, "MSS:", mss,
      "Rel. Err:", rel_err, "\n")
    rel_err_15 <- data.frame(iter, mss, rel_err)
    rel_err_15$missing_percent <- .15
    }
```

##### 20% Omit Rate

Here we will keep track of our error rates and number of iterations. This is for 20%.
```{r}
# Omit number of variables
nomit <- round(.20 * nrow(scaled_boston), 0)
set.seed(9)
ina <- sample(nrow(scaled_boston), nomit)
inb <- sample(1:ncol(scaled_boston), nomit, replace = TRUE)
Xna <- scaled_boston
index.na <- cbind(ina, inb)
Xna[index.na] <- NA

# Initialize xhat
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]

# Initialize thresholds
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)

while(rel_err > thresh) {
    iter <- iter + 1
    # Step 2(a)
    Xapp <- fit.svd(Xhat, m = 8)
    # Step 2(b)
    Xhat[ismiss] <- Xapp[ismiss]
    # Step 2(c)
    mss <- mean(((Xna - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss) / mss0
    mssold <- mss
    cat("Iter:", iter, "MSS:", mss,
      "Rel. Err:", rel_err, "\n")
    rel_err_20 <- data.frame(iter, mss, rel_err)
    rel_err_20$missing_percent <- .20
    }
```

##### 25% Omit Rate

Here we will keep track of our error rates and number of iterations. This is for 25%.
```{r}
# Omit number of variables
nomit <- round(.25 * nrow(scaled_boston), 0)
set.seed(9)
ina <- sample(nrow(scaled_boston), nomit)
inb <- sample(1:ncol(scaled_boston), nomit, replace = TRUE)
Xna <- scaled_boston
index.na <- cbind(ina, inb)
Xna[index.na] <- NA

# Initialize xhat
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]

# Initialize thresholds
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)

while(rel_err > thresh) {
    iter <- iter + 1
    # Step 2(a)
    Xapp <- fit.svd(Xhat, m = 8)
    # Step 2(b)
    Xhat[ismiss] <- Xapp[ismiss]
    # Step 2(c)
    mss <- mean(((Xna - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss) / mss0
    mssold <- mss
    cat("Iter:", iter, "MSS:", mss,
      "Rel. Err:", rel_err, "\n")
    rel_err_25 <- data.frame(iter, mss, rel_err)
    rel_err_25$missing_percent <- .25
    }
```

##### 30% Omit Rate

Here we will keep track of our error rates and number of iterations. This is for 30%.
```{r}
# Omit number of variables
nomit <- round(.30 * nrow(scaled_boston), 0)
set.seed(9)
ina <- sample(nrow(scaled_boston), nomit)
inb <- sample(1:ncol(scaled_boston), nomit, replace = TRUE)
Xna <- scaled_boston
index.na <- cbind(ina, inb)
Xna[index.na] <- NA

# Initialize xhat
Xhat <- Xna
xbar <- colMeans(Xna, na.rm = TRUE)
Xhat[index.na] <- xbar[inb]

# Initialize thresholds
thresh <- 1e-7
rel_err <- 1
iter <- 0
ismiss <- is.na(Xna)
mssold <- mean((scale(Xna, xbar, FALSE)[!ismiss])^2)
mss0 <- mean(Xna[!ismiss]^2)

while(rel_err > thresh) {
    iter <- iter + 1
    # Step 2(a)
    Xapp <- fit.svd(Xhat, m = 8)
    # Step 2(b)
    Xhat[ismiss] <- Xapp[ismiss]
    # Step 2(c)
    mss <- mean(((Xna - Xapp)[!ismiss])^2)
    rel_err <- (mssold - mss) / mss0
    mssold <- mss
    cat("Iter:", iter, "MSS:", mss,
      "Rel. Err:", rel_err, "\n")
    rel_err_30 <- data.frame(iter, mss, rel_err)
    rel_err_30$missing_percent <- .30
    }
```

##### Results

Here we are going to make a table of the results of the errors present.
```{r}
error_table <- rbind(rel_err_5, rel_err_10, rel_err_15, rel_err_20, rel_err_25, rel_err_30)
error_table <- error_table %>%
  dplyr::select(missing_percent, everything()) %>%
  as_tibble()
error_table
```

As we can see, as the missing percentage goes up, the mss goes down but the relative error goes up slightly. We can also see that 20% missing data rate causes the most iterations and time to process these numbers and the 5% missing data rate causes the least iterations and times to process these numbers.

### Problem 4 {.tabset}

#### Part A

Here we are going to read in the data for this problem.
```{r}
problem4_data <- as_tibble(read.csv("Ch12Ex13.csv", header = F))
```

#### Part B

Apply hierarchical clustering to the samples using correlation based distance, and plot the dendrogram.
```{r}
# Find distances
Distances <- dist(cor(problem4_data))

methods <- c("centroid", "average", "single", "complete")

par(mfrow = c(2, 2))
for (i in methods) {
    clusters <- hclust(Distances, method = i)
    
    plot(clusters, 
         col = "black", 
         col.main = "blue", 
         col.lab = "red", 
         col.axis = "green", 
         lwd = 1, 
         lty = 1, 
         cex = .69,
         sub = "Eric Warren", 
         hang = -1, 
         axes = FALSE,
         main = paste0("Cluster Dendrogram using\n", i,  " method"))
}
```

Here we can see that there are some extreme differences between the methods to perform clustering. We can see in all the method that there seems to be two distinctive groups. The type of linkage involved might affect the distances but it does not affect these two different groups. The Centroid method does have a weird looking second group.

#### Part C

Your collaborator wants to know which genes differ the most across the two groups. I am going to walk through a way to try to find it.

In order to see which variables differ the most between healthy and diseased patients I suggest the following:
-Perform PCA on the data
-Perform K-Means clustering on the first two dimensions and make plots to see if there is good separation on the components.
-If there is such a difference, we will test to see if one group of genes is the one we are looking for.

Here we will use PCA.
```{r}
set.seed(9)
par(mfrow = c(1, 1))
problem4_pca <- prcomp(problem4_data)

pca_data <- problem4_pca$x[ , 1:2] %>% 
  as_tibble()

pca_kmeans <- kmeans(pca_data, centers = 2)

plot(pca_data, 
     col = pca_kmeans$cluster,
     main = "PCA Vectors of the Different Groups",
     sub = "Eric Warren")
legend(-6, 3.8, legend = c("Cluster 1", "Cluster 2"),
       fill = c("red", "black"))
```

From this plot, we can see the two distinct different groups of genes that are present using what was described above.
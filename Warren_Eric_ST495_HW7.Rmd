---
title: "ST 495 HW5"
author: "Eric Warren"
date: "`r Sys.Date()`"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      fig.height = 7,
                      fig.width = 10)
```

## Honor Code {.tabset}

I have neither given nor received unauthorized assistance on this test or assignment. -**Eric Warren**

### Problem 1

Here we are going to consider the  Gini index, classification error, and entropy in a simple classification setting with two classes. Create a single plot that displays each of these quantities as a function of $\hat{p}_{m1}$. The x-axis should display $\hat{p}_{m1}$, ranging from 0 to 1, and the y-axis should display the value of the Gini index, classification error, and entropy.
```{r}
library(tidyverse)
p <- seq(0, 1, 0.01)
gini <- p * (1 - p) * 2
entropy <- -(p * log(p) + (1 - p) * log(1 - p))
classError <- 1 - pmax(p, 1 - p)
plotting <- as_tibble(cbind(p, gini, entropy, classError))
plotting %>%
  tidyr::gather("id", "value", 2:4) %>%
  ggplot(aes(x = p, y = value, color = id)) +
  geom_point(alpha = 0.3) +
  theme_bw() +
  labs(title = "How the Values Change Per Modeling Technique",
       caption = "Eric Warren",
       x = "P_m1",
       y = "Value",
       color = "Model Type") +
  scale_color_discrete(labels = c("Classification Error", "Entropy", "Gini")) +
  theme_bw() +
  theme(plot.title = element_text(color = "blue",
                                  face = "bold",
                                  size = 16,
                                  hjust = 0.5))
```

We can see that Classification Errors has the smallest values followed by Gini and then Entropy has the highest value.

### Problem 2

Here we are going to make a side by side diagram of:

A. Sketch the tree corresponding to the partition of the predictor space illustrated in the left-hand panel of Figure 8.14. The numbers inside the boxes indicate the mean of Y within each region.

B. Create a diagram similar to the left-hand panel of Figure 8.14, using the tree illustrated in the right-hand panel of the same figure. You should divide up the predictor space into the correct regions, and indicate the mean for each region.

These diagrams will be one on top of the other as Diagram A is on the top and Diagram B is on the bottom.
```{r, out.width = '100%', fig.cap = "Diagrams"}
knitr::include_graphics("./Eric_Warren_ST495_HW7_Problem2.png")
```

### Problem 3

Here we are going to create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for mtry and ntree.
```{r}
library(ISLR2)
library(MASS)
library(randomForest)
Boston <- as_tibble(Boston)
set.seed(9)
trainSplit <- sample(1:nrow(Boston), nrow(Boston) / 2)
train <- Boston[trainSplit, ]
test <- Boston[-trainSplit, ]
rf.boston1 <- randomForest(train, 
                           y = train$medv, 
                           xtest = test, 
                           ytest = test$medv, 
                           mtry = ncol(Boston) - 1, 
                           ntree = 5000)
rf.boston2 <- randomForest(train, 
                           y = train$medv, 
                           xtest = test, 
                           ytest = test$medv, 
                           mtry = (ncol(Boston) - 1) / 2, 
                           ntree = 5000)
rf.boston3 <- randomForest(train, 
                           y = train$medv, 
                           xtest = test, 
                           ytest = test$medv, 
                           mtry = (ncol(Boston) - 1) / 3, 
                           ntree = 5000)
rf.boston4 <- randomForest(train, 
                           y = train$medv, 
                           xtest = test, 
                           ytest = test$medv,
                           mtry = sqrt(ncol(Boston) - 1), 
                           ntree = 5000)
rf.boston5 <- randomForest(train, 
                           y = train$medv, 
                           xtest = test, 
                           ytest = test$medv,
                           mtry = (ncol(Boston) - 1) ** 2, 
                           ntree = 5000)

plot(1:5000, 
     rf.boston1$test$mse, 
     col = "green", 
     type = "l", 
     xlab = "Number of Trees", 
     ylab = "Test MSE", 
     ylim = c(0, 20)
     )
lines(1:5000, 
      rf.boston2$test$mse, 
      col = "red", 
      type = "l"
      )
lines(1:5000, 
      rf.boston3$test$mse, 
      col = "blue", 
      type = "l"
      )
lines(1:5000, 
      rf.boston4$test$mse, 
      col = "black", 
      type = "l"
      )
lines(1:5000, 
      rf.boston5$test$mse, 
      col = "brown", 
      type = "l"
      )
legend("topright", 
       c("m = p", 
         "m = p/2",
         "m = p/3",
         "m = sqrt(p)",
         "m = p^2"), 
       col = c("green", "red", "blue", "black", "brown"), 
       cex = 1, 
       lty = 1
       )
title(main = "MSE Values From Different Type of Random Forest Models",
      sub = "Eric Warren"
      )
```

Here we can see that $p^2$ and $p$ have the best MSE and thus the best models for the Boston data when predicting the median home value, with $p^2$ being slightly better. The $p/3$ and $\sqrt{p}$ parameters are the worst models to use due to the highest MSE, with the $p/3$ model being slightly worse.

### Problem 4 {.tabset}

#### Read in the data

Here we are going to read in the `OJ` data set.
```{r}
OJ <- as_tibble(OJ)
```

#### Part A

Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.
```{r}
set.seed(9)
trainingSplit <- sample(1:nrow(OJ), 800)
train <- OJ[trainingSplit, ]
test <- OJ[-trainingSplit, ]
```

#### Part B

Here we are going to fit a tree to the training data, with `Purchase` as the response and the other variables as predictors. Use the `summary()` function to produce summary statistics about the tree, and describe the results obtained.
```{r}
library(tree)
treeOJ <- tree(Purchase ~ ., train)
summary(treeOJ)

```

We can see that this tree has 8 terminal nodes and a misclassification error rate of `r summary(treeOJ)$misclass[1] / summary(treeOJ)$misclass[2]` or that in the training data it gets roughly `r summary(treeOJ)$misclass[1] / summary(treeOJ)$misclass[2] * 100`% of the predictions wrong. The most important variables or what the tree used in making its predictions are `LoyalCH`, `PriceDiff`, and `ListPriceDiff`.

#### Part C

Here we are going to type in the name of the tree object in order to get a detailed text output and then pick one of the terminal nodes, and interpret the information displayed.
```{r}
treeOJ
```

I picked the node labeled 9, which is a terminal node because of the asterisk. The split criterion is `LoyalCH` > 0.035047, the number of observations in this branch is 106 with a deviance of 99.690 and an overall prediction for the branch of MM. About 17.925% of the observations in that branch take the value of CH, and the remaining 82.075% take the value of MM.

#### Part D

Here I am going to create a plot of the tree, and interpret the results.
```{r}
plot(treeOJ)
text(treeOJ, pretty = 1)
```

The most important indicator of `Purchase` seems to be `LoyalCH`, since the first branch shows the intensity of something or the most important indictor. IN this cass the first branch differentiates the intensity of customer brand loyalty to CH. In fact, the top three nodes contain `LoyalCH`.

#### Part E

Here we are going to Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels.
```{r}
tree.pred <- predict(treeOJ, newdata = test, type = "class")
table.tree <- table(tree.pred, test$Purchase)
table.tree
accuracyTable <- caret::confusionMatrix(table.tree)
accuracyTable$overall[[1]]
```

We can see that this model produces a test observation accuracy level of `r round(accuracyTable$overall[[1]], 4)` or that roughly `r round(accuracyTable$overall[[1]] * 100, 2)`% of the test observations were correctly predicted. Thus, it has a test error rate of `r round((1 - accuracyTable$overall[[1]]) * 100, 2)`%.

#### Part F

Here we are going to apply the `cv.tree()` function to the training set in order to determine the optimal tree size.
```{r}
cvOJ <- cv.tree(treeOJ, FUN = prune.misclass)
cvOJ
```

Here we can see that the optimal tree size is `r cvOJ$size[order(cvOJ$dev, cvOJ$size)[1]]` and `r cvOJ$size[order(cvOJ$dev, cvOJ$size)[2]]`. We should probably pick `r cvOJ$size[order(cvOJ$dev, cvOJ$size)[1]]` since it uses the least number of nodes while providing the smallest error, but we are going to double check that in future parts of this problem.

#### Part G

Here we are going to produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.
```{r}
plot(cvOJ$size,
     cvOJ$dev,
     type = 'b', 
     xlab = "Tree size", 
     ylab = "Cross Validation Error"
     )
title(main = "How Tree Size Affects the Classification Error Rate",
      sub = "Eric Warren"
      )
```

Here we can visually see that a tree size of 6 through 8 look about the same for classification error rate.

#### Part H

Here we are going to find the tree size corresponds to the lowest cross-validated classification error rate.
```{r}
bestSize <- cvOJ$size[order(cvOJ$dev, cvOJ$size)[1]]
```

Here we can see the best size of the tree is a `r bestSize` node tree. We can also see from the graph that a `r cvOJ$size[order(cvOJ$dev, cvOJ$size)[2]]` node tree might work too.

#### Part I

Here we are going to Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.
```{r}
prune.OJ <- prune.misclass(treeOJ, best = bestSize)
plot(prune.OJ)
text(prune.OJ, pretty = 1)
```

This does lead to a pruned tree that we can see here. We used the best size which in this case was `r bestSize` terminal nodes.

#### Part J

Here we are going to compare the training error rates between the pruned and un-pruned trees.
```{r}
summary(treeOJ)
summary(prune.OJ)
```

We can see that the un-pruned tree has 8 terminal nodes and a misclassification error rate of `r summary(treeOJ)$misclass[1] / summary(treeOJ)$misclass[2]` or that in the training data it gets roughly `r summary(treeOJ)$misclass[1] / summary(treeOJ)$misclass[2] * 100`% of the predictions wrong. The most important variables or what the tree used in making its predictions are `LoyalCH`, `PriceDiff`, and `ListPriceDiff`. For the pruned tree it has 6 terminal nodes and a misclassification error rate of `r summary(prune.OJ)$misclass[1] / summary(prune.OJ)$misclass[2]` or that in the training data it gets roughly `r summary(prune.OJ)$misclass[1] / summary(prune.OJ)$misclass[2] * 100`% of the predictions wrong. The most important variables or what the tree used in making its predictions are `LoyalCH` and `PriceDiff`. As we can see they have the same training error rate.

#### Part K

Here we are going to compare the test error rates between the pruned and un-pruned trees.
```{r}
# Un-pruned tree
tree.pred <- predict(treeOJ, newdata = test, type = "class")
table.tree <- table(tree.pred, test$Purchase)
table.tree
accuracyTable <- caret::confusionMatrix(table.tree)
accuracyTable$overall[[1]]

# Pruned tree
pruned.tree.pred <- predict(prune.OJ, newdata = test, type = "class")
pruned.table.tree <- table(pruned.tree.pred, test$Purchase)
pruned.table.tree
pruned.accuracyTable <- caret::confusionMatrix(pruned.table.tree)
pruned.accuracyTable$overall[[1]]
```

We can see that the un-pruned model produces a test observation accuracy level of `r round(accuracyTable$overall[[1]], 4)` or that roughly `r round(accuracyTable$overall[[1]] * 100, 2)`% of the test observations were correctly predicted. Thus, it has a test error rate of `r round((1 - accuracyTable$overall[[1]]) * 100, 2)`%.

We can see that the pruned model produces a test observation accuracy level of `r round(pruned.accuracyTable$overall[[1]], 4)` or that roughly `r round(pruned.accuracyTable$overall[[1]] * 100, 2)`% of the test observations were correctly predicted. Thus, it has a test error rate of `r round((1 - pruned.accuracyTable$overall[[1]]) * 100, 2)`%.

In conclusion, we can see both have the same test error rate as well. This makes sense since the pruned and un-pruned model both had the same deviance. We should use the pruned model, because it will produce more interpretative tree that we can use for our predictions.